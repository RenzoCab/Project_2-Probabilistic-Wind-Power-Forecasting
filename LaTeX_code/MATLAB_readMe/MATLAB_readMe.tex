\PassOptionsToPackage{table}{xcolor}
\documentclass[aspectratio=169]{beamer}\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{amsmath,mathtools}
\usepackage{booktabs}
\usepackage{mathptmx}
\usepackage[11pt]{moresize}
\usepackage{hyperref}
\usepackage{commath}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{siunitx}
\usepackage{multirow}

\usepackage{listings}
\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}

\setbeamertemplate{navigation symbols}{}
\setbeamersize{text margin left=5mm,text margin right=5mm}
\setbeamertemplate{caption}[numbered]
\addtobeamertemplate{navigation symbols}{}{
\usebeamerfont{footline}
\usebeamercolor[fg]{footline}
\hspace{1em}
\insertframenumber/\inserttotalframenumber}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\I}{\mathbb{I}}

\title{MATLAB: Read Me}
\subtitle{Renzo Miguel Caballero Rosas}

\begin{document}

\lstset{language=Matlab,
    breaklines=true,
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},
    showstringspaces=false,
    numbers=left,%
    numberstyle={\tiny \color{black}},
    numbersep=9pt,
    emph=[1]{for,end,break},emphstyle=[1]\color{red},  
}

\begin{frame}
\titlepage
\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Questions:}

\begin{itemize}

\item Why $\dif X=a(X;\alert{\theta})\dif t+b(X;\alert{\theta},\alert{\alpha})\dif W$ and no $\dif X=a(X;\alert{\theta})\dif t+b(X;\alert{\gamma})\dif W$? Where all $\alert{\theta},\alert{\alpha},\alert{\gamma}\in\R^+$. {\color{green}Because in the way it is defined, $\theta$ controls the mean reversion and $\alpha$ the wide of the confidence band.} {\color{red} However, maybe it is better to optimize over $\theta$ and $\gamma$? Because of the relative dimension. After, trivially we can compute $\alpha=\gamma/\theta$}.

\item Which is Beta, the measurements or the transitions? 

\item Which data in the histograms? Measurements or transitions? 

\end{itemize}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Some keywords:}

\begin{itemize}

\item Our process is: \textbf{High-frequency in a fixed time-interval}.

\end{itemize}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Normalization:}

Given the SDE
\begin{equation*}
\dif V_t=-\theta_tV_t\dif t+\sqrt{2\theta_t\alpha(V_t+p_t)(1-V_t-p_t)}\dif W_t,
\end{equation*}
we consider the normalized differentials $\dif\hat{t}=\frac{\dif t}{T}$, and $\dif\hat{W}_t=\frac{\dif W_t}{\sqrt{T}}$. Then, we can write the SDE as
\begin{equation*}
\dif V_t=-\alert{\theta_tT}V_t\dif\hat{t}+\sqrt{2\alert{\theta_tT}\alpha(V_t+p_t)(1-V_t-p_t)}\dif\hat{W}_t.
\end{equation*}
We conclude that, whatever the normalization constant T is, it gets absorbed by the parameter $\theta_t$ (let $\hat{\theta}_t=\theta_t T$).
\quad\\
The E-M representation is
\begin{equation*}
V_{t_{n+1}}=V_{t_{n}}-\left[\hat{\theta}_{t_n}V_{t_n}\right]\Delta s+\left[\sqrt{2\hat{\theta}_{t_n}\alpha(V_{t_n}+p_{t_n})(1-V_{t_n}-p_{t_n})}\right]\sqrt{\Delta s}\Delta\hat{W}_{t_n},\ V_{t_0}=v_0,
\end{equation*}
for $n\in\{0\dots,m-1\}$, $t_j=t_0+j\Delta s$, $t_0=0$, $t_m=1$, and $\Delta\hat{W}_{t_n}$ normal (0,1) for each $t_n$.

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{SDE first moment (1/2):} \label{m1}

Given some measurement $v_{t_{n-1}}$, we want to compute the first moment at time $t_n$. The exact first moment $m_1(s)$ for $s\in[t_{n-1},t_n]$ is the solution of the ODE

\begin{equation*}
\begin{cases}
\dif m_1(s)=\left[-m_1(s)\theta(s)\right]\dif s,\\
m_1(t_{n-1})=v_{t_{n-1}}.
\end{cases}
\end{equation*}
If $\theta(t_{n-1})=\theta(t_{n})=\theta$, the solution is $m_1(t_n)=m_1(t_{n-1})e^{-\theta(t_n-t_{n-1})}$.\\
\quad\\
Otherwise, we compute a linear interpolation for $\theta(s)$ and solve the ODE using Forward-Euler:
\begin{equation*}
m_1(s_{n})=m_1(s_{n-1})(1-\theta(s_{n-1})\Delta s).
\end{equation*}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{SDE first moment (2/2): CODE} \label{m1_code}

\begin{center}
\begin{tabular}{|c|}
\toprule
{\footnotesize
\lstinputlisting{../moment_1.m}
}\\
\bottomrule
\end{tabular}
\end{center}

The code is automatically imported from the MATLAB script.

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{SDE second moment (1/2):} \label{m2}

Given some measurement $v_{t_{n-1}}$, we want to compute the second moment at time $t_n$. The exact second moment $m_2(s)$ for $s\in[t_{n-1},t_n]$ is the solution of the ODE

\begin{equation*}
\begin{cases}
\dif m_2(s)&=\alert{\left[-2(1+\alpha)m_2(s)\theta(s)+2\alpha\theta(s)m_1(s)(1-2p(s))+2\alpha\theta(s)p(s)(1-p(s))\right]\dif s,}\\
&=2\theta(s)\left[-(1+\alpha)m_2(s)+\alpha m_1(s)(1-2p(s))+\alpha p(s)(1-p(s))\right]\dif s,\\
m_2(t_{n-1})&=v^2_{t_{n-1}}.
\end{cases}
\end{equation*}
We compute a linear interpolation for the functions $\theta(s)$ and $p(s)$. After, we solve the ODE using Forward-Euler:
{\scriptsize
\begin{equation*}
m_2(s_n)=m_2(s_{n-1})+2\theta(s_{n-1})\left[-(1+\alpha)m_2(s_{n-1})+\alpha m_1(s_{n-1})(1-2p(s_{n-1}))+\alpha p(s_{n-1})(1-p(s_{n-1}))\right]\Delta s.
\end{equation*}}
We use the same discretization points for both $m_1(s)$ and $m_2(s)$.

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{SDE second moment (2/2): CODE} \label{m2_code}

\begin{center}
\begin{tabular}{|c|}
\toprule
{\footnotesize
\lstinputlisting{../moment_2.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}

\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Density next measurement (1/2):} \label{Tr}

We want the next measurement $V_{t_n}|V_{t_{n-1}}$ to have a Beta distribution, but with support in $[a,b]=[-1,1]$. Given $X\sim\beta(\xi_1,\xi_2)$ a \alert{Beta distributed random variable}, we define the new random variable $V=a+(b-a)X$ with support in $[-1,1]$, and PDF $f_V(v)$.\\
\quad\\
We can compute:
\begin{itemize}
\item $\E[V]=a+(b-a)\E[X]=a+(b-a)\frac{\xi_1}{\xi_1+\xi_2}=\mu_V$.
\item $\V[V]=(b-a)^2\V[X]=\frac{(b-a)^2\xi_1\xi_2}{(\xi_1+\xi_2)^2(\xi_1+\xi_2+1)}=\sigma^2_V$.
\end{itemize}
\quad\\
\quad\\
Then, we want the SDE and our new PDF $f_V(v)$ to have the same moments at each $t\in\{\text{some appropriate domain}\}$, i.e., $\mu(t)=m_1(t)$ and $\sigma^2(t)=m_2(t)-m_1^2(t)$.\\
$\mu(t)$ and $\sigma^2(t)$ refers to the mean and variance of $V_{t_n}|V_{t_{n-1}}$, following the structure described for $f_V(v)$.

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Density next measurement (2/2):}

For each measurement $V_{t_{n-1}}$, we can find the analytical moments for the SDE at time $t_n$ solving the ODEs from slides {\color{blue}\ref{m1}} and {\color{blue}\ref{m2}}. Then, we can find the parameters $\xi_1$ and $\xi_2$ such that both the SDE and the PDF of $V_{t_n}|V_{t_{n-1}}$ have the same first and second moments at time $t_n$.
\begin{itemize}
\item $\xi_1=-\frac{(1+\mu)(\mu^2+\sigma^2-1)}{2\sigma^2}$ all evaluated at time $t_n$ (verified in \textbf{Mathematica 11.0}\footnote{File: {\color{blue}matchingVerification.nb}.}).
\item $\xi_2=\frac{(\mu-1)(\mu^2+\sigma^2-1)}{2\sigma^2}$ all evaluated at time $t_n$ (verified in \textbf{Mathematica 11.0}).
\end{itemize}

\begin{center}
\begin{tabular}{|c|}
\toprule
{\footnotesize
\lstinputlisting{../moments_matching.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Log-density (1/2):}

Recall the PDF $f_V(v)$ from slide {\color{blue}\ref{Tr}}. We will use this density to model the random variables $V_{t_n}|V_{t_{n-1}}$. For $[a,b]=[-1,1]$, we have that
\begin{equation*}
f_V(v)=f_X(g^{-1}(v))\left|\frac{\dif}{\dif v}g^{-1}(v)\right|\quad \text{where}\quad f_X(x)=\text{Beta}(\xi_1,\xi_2)\quad\text{and}\quad g(x)=a+(b-a)x.
\end{equation*}
Then, $f_V(v)=\frac{1}{|(b-a)|}\frac{1}{B(\xi_1,\xi_2)}\left(\frac{v-a}{b-a}\right)^{\xi_1-1}\left(1-\frac{v-a}{b-a}\right)^{\xi_2-1}$ because $g^{-1}(v)=\frac{v-a}{b-a}$.\\
\quad\\
Also, we have that (up to some constant values)
\begin{equation*}
\log\left(f_V(v)\right)=\log\left(\frac{1}{B(\xi_1,\xi_2)}\right)+(\xi_1-1)\log\left(\frac{v-a}{b-a}\right)+(\xi_2-1)\log\left(\frac{b-v}{b-a}\right),
\end{equation*}
where $\xi_1$ and $\xi_2$ depends on the SDE moments.

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Log-density (2/2): CODE}

\begin{center}
\begin{tabular}{|c|}
\toprule
{\footnotesize
\lstinputlisting{../log_dist.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Log-likelihood (1/2):}

We introduce the number of paths $M$, and the number of measurements per path $N+1$ ($N$ transitions). Then, we have a total of $M\times N$ samples to use. Notice that each pair $(\xi_1,\xi_2)$ depends on $i\in\{1,\dots,M\}$ and $j\in\{2,\dots,N+1\}$. Then, the log-likelihood is
\begin{equation*}
\mathfrak{L}\left(\{V\}_{M,N}\right)=\sum_{i=1}^M\sum_{j=2}^{N+1}\log\left[\rho_{i,j}\left(V_{i,j}|V_{i,j-1}\right)\right],
\end{equation*}
where $\rho_{i,j}\left(V_{i,j}|V_{i,j-1}\right)=\rho_{i,j}\left(V_{i,j}|V_{i,j-1};\xi_{1_{i,j}},\xi_{2_{i,j}}\right)$, and where we assumed a non-informative prior.

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Data: CODE}

We load our three tables: \textbf{Table\_Training\_Complete}, \textbf{Table\_Testing\_Complete}, and \textbf{Table\_Complete}.

\begin{center}
\begin{tabular}{|c|}
\toprule
{\tiny
\lstinputlisting{../load_data.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Create a new batch (1/2):} \label{cre_batch}

To guarantee the data homogeneity, we sample per day and not per transition. This means that each batch is composed of data corresponding to some amount of days. If we sample a total of $Z\in\N$ days, the batch corresponding to this days is
\begin{table}[]
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{7}{|l|}{PATH 1}                                                                                       & ...                  & PATH Z               \\ \hline
\multicolumn{2}{|l|}{$t_n=01:10$}   & \multicolumn{2}{l|}{$t_n=01:20$}    & ... & \multicolumn{2}{l|}{$t_n=00:50$} & \multirow{4}{*}{...} & \multirow{4}{*}{...} \\ \cline{1-7}
$p(t_{n-1})$      & $p(t_{n})$      & $p(t_{n-1})$      & $p(t_{n})$      & ... & $p(t_{n-1})$     & $p(t_{n})$    &                      &                      \\ \cline{1-7}
$\dot{p}(t_{n-1})$ & $\dot{p}(t_{n})$ & $\dot{p}(t_{n-1})$ & $\dot{p}(t_{n})$ & ... & $\dot{p}(t_{n-1})$     & $\dot{p}(t_{n})$    &                      &                      \\ \cline{1-7}

$V(t_{n-1})$      & $V(t_{n})$      & $V(t_{n-1})$      & $V(t_{n})$      & ... & $V(t_{n-1})$     & $V(t_{n})$    &                      &                      \\ \hline
\end{tabular}
\end{table}
with dimensions $3\times(2Z(N-1))$. As an example: If we have 145 measurements (N+1), then $N=144$ and $N-1=143$. We use 143 samples because we need to ignore the initial measurement (because we do not have data at time $t_{-1}$) and the final one (because it does not have $\dot{p}$). Then, each day has 143 samples. In this implementation, we are duplicating the data. In case of a lack of RAM, we can reduce the dimensions to $3\times(Z(N-1))$.
\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Create a new batch (2/2): CODE}

\begin{center}
\begin{tabular}{|c|}
\toprule
{\tiny
\lstinputlisting{../new_batch.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{From $\theta_0$ to $\theta_t$:}

To ensure that the analytical solutions is always in $[0,1]$, we choose the drift parameter to be
\begin{equation*}
\theta(t) = \max\left(\theta_0,\frac{|\dot{p}(t)|}{\min(p(t),1-p(t))}\right),\quad\theta_0>0.
\end{equation*}
\quad\\
\quad\\
\begin{center}
\begin{tabular}{|c|}
\toprule
{\tiny
\lstinputlisting{../theta_t.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Complete batch:}

After we created a batch with the elements $(p(t),\dot{p}(t),V(t))$, we want to add the parameter $\theta(t)$ to use in the likelihood. Following the same idea than in slide {\color{blue}\ref{cre_batch}}, the complete batch is
\begin{table}[]
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{7}{|l|}{PATH 1}                                                                                       & ...                  & PATH Z               \\ \hline
\multicolumn{2}{|l|}{$t_n=01:10$}   & \multicolumn{2}{l|}{$t_n=01:20$}    & ... & \multicolumn{2}{l|}{$t_n=00:50$} & \multirow{4}{*}{...} & \multirow{4}{*}{...} \\ \cline{1-7}
$p(t_{n-1})$      & $p(t_{n})$      & $p(t_{n-1})$      & $p(t_{n})$      & ... & $p(t_{n-1})$     & $p(t_{n})$    &                      &                      \\ \cline{1-7}
$\dot{p}(t_{n-1})$ & $\dot{p}(t_{n})$ & $\dot{p}(t_{n-1})$ & $\dot{p}(t_{n})$ & ... & $\dot{p}(t_{n-1})$     & $\dot{p}(t_{n})$    &                      &                      \\ \cline{1-7}

$V(t_{n-1})$      & $V(t_{n})$      & $V(t_{n-1})$      & $V(t_{n})$      & ... & $V(t_{n-1})$     & $V(t_{n})$    &                      &                      \\ \hline
$\theta(t_{n-1})$      & $\theta(t_{n})$      & $\theta(t_{n-1})$      & $\theta(t_{n})$      & ... & $\theta(t_{n-1})$     & $\theta(t_{n})$    &                      &                      \\ \hline

\end{tabular}
\end{table}
\begin{center}
\begin{tabular}{|c|}
\toprule
{\tiny
\lstinputlisting{../batch_with_theta.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Initial guess for $\theta_0\cdot\alpha$:}
Recall we have $M$ paths with $N+1$ measurements each.
\begin{equation*}
\theta_0 \alpha\approx \frac{1}{2M\Delta t} \sum\limits_{j=1}^M \frac{ \sum\limits_{i=1}^{N} (x_{i+1,j}  - x_{i,j})^2}{\sum\limits_{i=1}^{N} x_{i,j}(1-x_{i,j}) }\alert{\approx 0.021}.
\end{equation*}

\begin{center}
\begin{tabular}{|c|}
\toprule
{\tiny
\lstinputlisting{../initial_guess.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Initial guess for $\theta_0$:}
Recall we have $M$ paths with $N+1$ measurements each.
\begin{equation*}
\theta_0\approx\arg\min_{\theta_0}\left[\sum_{j=1}^M\sum_{i=1}^N\left(v_{i+1,j}-v_{i,j}+\theta_{t_i}v_{i,j}\Delta t\right)^2\right].
\end{equation*}

\begin{center}
\begin{tabular}{|c|}
\toprule
{\tiny
\lstinputlisting{../initial_theta.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Initial guess for $\theta_0$:}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.45\textwidth]{theta_0.eps}
\end{figure}

\alert{$\theta_0\approx 1.1450
$ and $\theta_0\alpha\approx 0.021\implies\alpha\approx0.018$.}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Log-likelihood evaluation: CODE}

\begin{center}
\begin{tabular}{|c|}
\toprule
{\tiny
\lstinputlisting{../log_LH_evaluation.m}
}\\
\bottomrule
\end{tabular}
\end{center}

\end{frame}


\setbeamercolor{background canvas}{bg=white!10}
\begin{frame}\frametitle{Log-likelihood: Plot}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.45\textwidth]{../log_likelihood.eps}\quad\includegraphics[width=0.45\textwidth]{../log_likelihood_2.eps}
\end{figure}

\end{frame}

\end{document}