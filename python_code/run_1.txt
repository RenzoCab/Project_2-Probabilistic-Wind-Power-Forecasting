 loading configuration 
Data output will be save in  fit_SDE19-10-01-12-40-43
Computing Hessian at intial point
Evaluating using rand_approx_lamperti
   0    11.000000000000    0.400000000000   -2.7
 optimizing rand_approx_lamperti using Nelder-Mead with 5 batches.
starting optimization
Evaluating using rand_approx_lamperti
   1    11.000000000000    0.400000000000    5.1
   2    11.000000000000    0.420000000000   -4.3
   3    10.450000000000    0.420000000000    10.1
   4    10.725000000000    0.415000000000    2.0
   5    10.725000000000    0.435000000000    5.7
   6    10.931250000000    0.408750000000   -41.2
   7    11.206250000000    0.413750000000   -19.6
/home/alhaddwt/anaconda3/lib/python3.7/site-packages/scipy/integrate/_ivp/rk.py:140: RuntimeWarning: invalid value encountered in maximum
  scale = atol + np.maximum(np.abs(y), np.abs(y_new)) * rtol
   8    11.034375000000    0.415625000000   -13.1
   9    11.051562500000    0.413437500000    9.0
  10    11.068750000000    0.411250000000    1.5
  11    10.982812500000    0.412187500000    6.9
  12    11.017187500000    0.407812500000   -31.6
  13    10.879687500000    0.405312500000    5.0
  14    11.021484375000    0.409765625000    2.5
  15    10.974218750000    0.408281250000    6.6
  16    11.000000000000    0.410000000000    7.6
  17    10.905468750000    0.407031250000    5.2
  18    10.862500000000    0.407500000000   -9.5
  19    10.888281250000    0.409218750000   -5.3
  20    10.896875000000    0.408125000000   -32.0
  21    10.916210937500    0.408164062500   -33.7
  22    10.950585937500    0.408789062500    1.9
  23    10.910302734375    0.408291015625    7.6
  24    10.923730468750    0.408457031250    5.5
  25    10.914062500000    0.408437500000   -32.1
  26    10.923193359375    0.408525390625   -32.0
  27    10.922119140625    0.408662109375    5.9
  28    10.922924804687    0.408559570313   -29.4
  29    10.922656250000    0.408593750000   -5.0
  30    10.927221679687    0.408637695312   -43.4
  31    10.935815429687    0.408793945313    6.7
  32    10.925946044922    0.408643798828    4.7
  33    10.924938964844    0.408615722656    6.0
  34    10.922924804687    0.408559570313    3.4
  35    10.925207519531    0.408581542969    6.0
  36    10.925140380859    0.408590087891    8.1
  37    10.926080322266    0.408626708984    7.4
  38    10.928228759766    0.408665771484   -22.3
  39    10.929370117187    0.408676757813    5.3
  40    10.928547668457    0.408664245605   -67.9
  41    10.927540588379    0.408636169434   -16.2
  42    10.928056716919    0.408658370972    6.0
  43    10.927884674072    0.408650970459    5.3
  44    10.928388214111    0.408665008545   -25.8
  45    10.928176307678    0.408657798767    6.6
  46    10.928216171265    0.408657608032    4.9
  47    10.928295898438    0.408657226563    5.9
  48    10.928338909149    0.408659076691    4.8
  49    10.928670406342    0.408665714264   -16.7
  50    10.928713417053    0.408667564392   -36.6
  51    10.928619790077    0.408665442467   -45.9
  52    10.928454041481    0.408662123680    5.9
  53    10.928648573160    0.408666204214    6.3
  54    10.928630542755    0.408665904999   -4.4
  55    10.928594481945    0.408665306568   -2.6
  56    10.928591793776    0.408665190935    6.7
  57    10.928565698862    0.408664544821   -13.2
  58    10.928524261713    0.408663715124   -29.4
  59    10.928550831974    0.408664262593   -30.4
  60    10.928574238718    0.408664793074   -32.1
  61    10.928571075201    0.408664776087   -46.2
  62    10.928544504941    0.408664228618    5.3
  63    10.928566805273    0.408664651960   -5.5
  64    10.928559371829    0.408664510846   -6.6
  65    10.928560953587    0.408664519340    9.2
  66    10.928546086699    0.408664237112   -30.9
  67    10.928546877578    0.408664241359    5.5
  68    10.928541025892    0.408664108738   -26.6
  69    10.928541816771    0.408664112985   -2.8
  70    10.928543081973    0.408664145079    8.4
  71    10.928547273017    0.408664243482    5.2
  72    10.928550594300    0.408664311916   -9.5
  73    10.928548202198    0.408664261121   -14.5
  74    10.928549264814    0.408664282640   -10.3
  75    10.928546605841    0.408664224087    5.4
  76    10.928547137149    0.408664234846   -29.0
  77    10.928546870279    0.408664227088   -32.2
  78    10.928547203258    0.408664235597   -8.8
  79    10.928547269368    0.408664236347   -12.9
  80    10.928547402803    0.408664240226   -29.0
  81    10.928547402499    0.408664239631   -21.3
  82    10.928547668761    0.408664246200    7.2
  83    10.928547469065    0.408664241274    7.8
  84    10.928547535630    0.408664242916    9.8
  85    10.928547535478    0.408664242618   -4.6
  86    10.928547668305    0.408664245308    6.2
  87    10.928547635136    0.408664244710   -0.9
  88    10.928547601968    0.408664244112    7.6
  89    10.928547651797    0.408664245158   -6.9
  90    10.928547718286    0.408664246651   -3.2
  91    10.928547660127    0.408664245382   -60.4
  92    10.928547635212    0.408664244859   -30.2
  93    10.928547649752    0.408664245176    7.3
  94    10.928547651835    0.408664245232    7.6
  95    10.928547656000    0.408664245344    0.1
  96    10.928547672622    0.408664245717    6.1
  97    10.928547662228    0.408664245475   -13.8
  98    10.928547660146    0.408664245419   -34.2
  99    10.928547663265    0.408664245493   -14.6
 100    10.928547665338    0.408664245531    5.5
 101    10.928547663783    0.408664245503   -2.7
 102    10.928547665861    0.408664245549   -31.8
 103    10.928547665730    0.408664245545   -29.1
 104    10.928547668588    0.408664245610    5.4
 105    10.928547666445    0.408664245561    6.6
 106    10.928547667159    0.408664245577   -40.0
 107    10.928547667094    0.408664245575    5.2
 108    10.928547668522    0.408664245608   -13.0
 109    10.928547668165    0.408664245600    5.3
 110    10.928547667775    0.408664245590   -1.9
 111    10.928547668424    0.408664245604   -29.2
 112    10.928547668108    0.408664245598   -45.5
 113    10.928547668353    0.408664245603    8.2
 114    10.928547668283    0.408664245602    5.3
 115    10.928547668441    0.408664245605   -6.8
 116    10.928547668615    0.408664245609   -40.7
 117    10.928547668632    0.408664245609    2.0
 118    10.928547668488    0.408664245606   -39.0
 119    10.928547668584    0.408664245608   -6.0
 120    10.928547668512    0.408664245607   -15.3
 121    10.928547668536    0.408664245607    8.7
 122    10.928547668473    0.408664245606   -60.5
 123    10.928547668394    0.408664245604   -11.0
 124    10.928547668465    0.408664245606    5.3
 125    10.928547668497    0.408664245606    4.8
 126    10.928547668461    0.408664245606   -55.2
 127    10.928547668441    0.408664245605    7.8
 128    10.928547668450    0.408664245605    7.0
 129    10.928547668468    0.408664245606    1.2
 130    10.928547668462    0.408664245606   -12.0
 131    10.928547668461    0.408664245606   -29.7
 132    10.928547668455    0.408664245605   -20.0
 133    10.928547668457    0.408664245605   -31.0
 134    10.928547668453    0.408664245605   -3.0
 135    10.928547668459    0.408664245606    7.0
 136    10.928547668459    0.408664245606   -5.0
 137    10.928547668459    0.408664245606    5.3
 138    10.928547668458    0.408664245605   -19.8
 139    10.928547668457    0.408664245605   -11.9
 140    10.928547668457    0.408664245605    5.4
 141    10.928547668458    0.408664245605    5.4
 142    10.928547668457    0.408664245605    5.3
 143    10.928547668456    0.408664245605   -26.2
 144    10.928547668456    0.408664245605   -9.0
 145    10.928547668457    0.408664245605    8.0
 146    10.928547668457    0.408664245605    6.6
 147    10.928547668457    0.408664245605    5.4
 148    10.928547668457    0.408664245605    8.5
 149    10.928547668457    0.408664245605    6.0
 150    10.928547668457    0.408664245605    6.3
 151    10.928547668457    0.408664245605   -8.2
 152    10.928547668457    0.408664245605    2.2
 153    10.928547668457    0.408664245605   -2.0
 154    10.928547668457    0.408664245605    6.4
 155    10.928547668457    0.408664245605   -0.0
 156    10.928547668457    0.408664245605    7.8
 157    10.928547668457    0.408664245605   -9.9
 158    10.928547668457    0.408664245605   -17.5
 159    10.928547668457    0.408664245605    5.8
 160    10.928547668457    0.408664245605    6.4
 161    10.928547668457    0.408664245605    2.5
 162    10.928547668457    0.408664245605    6.3
 163    10.928547668457    0.408664245605    6.0
 164    10.928547668457    0.408664245605   -5.7
 165    10.928547668457    0.408664245605    6.9
 166    10.928547668457    0.408664245605    5.5
 167    10.928547668457    0.408664245605    13.1
 168    10.928547668457    0.408664245605   -29.4
 169    10.928547668457    0.408664245605   -46.3
 170    10.928547668457    0.408664245605   -5.0
 171    10.928547668457    0.408664245605   -30.8
 172    10.928547668457    0.408664245605    5.8
 173    10.928547668457    0.408664245605   -5.8
 174    10.928547668457    0.408664245605   -1.3
 175    10.928547668457    0.408664245605   -4.7
 176    10.928547668457    0.408664245605    8.4
 177    10.928547668457    0.408664245605   -1.4
 178    10.928547668457    0.408664245605   -25.2
 179    10.928547668457    0.408664245605   -0.9
 180    10.928547668457    0.408664245605    4.9
 181    10.928547668457    0.408664245605    6.1
 182    10.928547668457    0.408664245605   -31.9
 183    10.928547668457    0.408664245605   -0.4
 184    10.928547668457    0.408664245605    6.3
 185    10.928547668457    0.408664245605    5.6
 186    10.928547668457    0.408664245605   -4.7
 187    10.928547668457    0.408664245605   -36.0
 188    10.928547668457    0.408664245605    5.4
 189    10.928547668457    0.408664245605   -49.0
 190    10.928547668457    0.408664245605    2.1
 191    10.928547668457    0.408664245605   -57.0
 192    10.928547668457    0.408664245605   -31.5
 193    10.928547668457    0.408664245605    5.2
 194    10.928547668457    0.408664245605    5.3
 195    10.928547668457    0.408664245605   -0.7
 196    10.928547668457    0.408664245605    10.5
 197    10.928547668457    0.408664245605    7.4
 198    10.928547668457    0.408664245605    4.6
 199    10.928547668457    0.408664245605    8.5
 200    10.928547668457    0.408664245605   -52.0
 201    10.928547668457    0.408664245605    5.3
 202    10.928547668457    0.408664245605    1.2
 203    10.928547668457    0.408664245605    6.5
 204    10.928547668457    0.408664245605   -53.2
 205    10.928547668457    0.408664245605   -28.3
 206    10.928547668457    0.408664245605    4.3
 207    10.928547668457    0.408664245605    5.6
 208    10.928547668457    0.408664245605    8.2
 209    10.928547668457    0.408664245605   -11.5
 210    10.928547668457    0.408664245605    8.5
 211    10.928547668457    0.408664245605   -3.6
 212    10.928547668457    0.408664245605   -26.6
 213    10.928547668457    0.408664245605    7.1
 214    10.928547668457    0.408664245605    8.0
 215    10.928547668457    0.408664245605    5.6
 216    10.928547668457    0.408664245605    5.3
 217    10.928547668457    0.408664245605    3.7
 218    10.928547668457    0.408664245605   -24.1
 219    10.928547668457    0.408664245605    8.2
 220    10.928547668457    0.408664245605    6.3
 221    10.928547668457    0.408664245605   -3.9
 222    10.928547668457    0.408664245605    0.0
 223    10.928547668457    0.408664245605   -45.2
 224    10.928547668457    0.408664245605    6.7
 225    10.928547668457    0.408664245605    5.2
 226    10.928547668457    0.408664245605   -3.2
 227    10.928547668457    0.408664245605   -30.3
 228    10.928547668457    0.408664245605   -1.6
 229    10.928547668457    0.408664245605    6.6
 230    10.928547668457    0.408664245605    4.1
 231    10.928547668457    0.408664245605    3.1
 232    10.928547668457    0.408664245605    4.3
 233    10.928547668457    0.408664245605    6.4
 234    10.928547668457    0.408664245605    8.9
 235    10.928547668457    0.408664245605    6.3
 236    10.928547668457    0.408664245605    5.4
 237    10.928547668457    0.408664245605    7.1
 238    10.928547668457    0.408664245605   -61.3
 239    10.928547668457    0.408664245605   -4.4
 240    10.928547668457    0.408664245605    8.3
 241    10.928547668457    0.408664245605   -18.7
 242    10.928547668457    0.408664245605    1.8
 243    10.928547668457    0.408664245605    5.9
 244    10.928547668457    0.408664245605    4.1
 245    10.928547668457    0.408664245605    6.4
 246    10.928547668457    0.408664245605    6.3
 247    10.928547668457    0.408664245605    3.6
 248    10.928547668457    0.408664245605    6.1
 249    10.928547668457    0.408664245605    6.2
 250    10.928547668457    0.408664245605    8.3
 251    10.928547668457    0.408664245605    0.2
 252    10.928547668457    0.408664245605    2.9
 253    10.928547668457    0.408664245605    5.8
 254    10.928547668457    0.408664245605    5.1
 255    10.928547668457    0.408664245605   -30.8
 256    10.928547668457    0.408664245605    5.8
 257    10.928547668457    0.408664245605   -30.8
 258    10.928547668457    0.408664245605    1.5
 259    10.928547668457    0.408664245605    5.5
 260    10.928547668457    0.408664245605    5.5
 261    10.928547668457    0.408664245605    6.3
 262    10.928547668457    0.408664245605   -29.3
 263    10.928547668457    0.408664245605    3.1
 264    10.928547668457    0.408664245605   -33.5
 265    10.928547668457    0.408664245605    8.0
 266    10.928547668457    0.408664245605    2.6
 267    10.928547668457    0.408664245605   -19.3
 268    10.928547668457    0.408664245605    5.2
 269    10.928547668457    0.408664245605   -32.2
 270    10.928547668457    0.408664245605   -30.7
 271    10.928547668457    0.408664245605    7.6
 272    10.928547668457    0.408664245605   -0.1
 273    10.928547668457    0.408664245605   -22.3
 274    10.928547668457    0.408664245605    6.1
 275    10.928547668457    0.408664245605   -2.6
 276    10.928547668457    0.408664245605    5.5
 277    10.928547668457    0.408664245605    6.3
 278    10.928547668457    0.408664245605   -30.2
 279    10.928547668457    0.408664245605   -7.8
 280    10.928547668457    0.408664245605   -3.3
 281    10.928547668457    0.408664245605   -6.9
 282    10.928547668457    0.408664245605   -6.4
 283    10.928547668457    0.408664245605    5.0
 284    10.928547668457    0.408664245605    0.5
 285    10.928547668457    0.408664245605   -30.8
 286    10.928547668457    0.408664245605    5.5
 287    10.928547668457    0.408664245605   -22.5
 288    10.928547668457    0.408664245605    5.7
 289    10.928547668457    0.408664245605   -61.8
 290    10.928547668457    0.408664245605    2.2
 291    10.928547668457    0.408664245605    6.5
 292    10.928547668457    0.408664245605    5.4
Warning: Maximum number of function evaluations has been exceeded.
 293    11.182464002644    0.851950659804   -111.0
 294    11.181918037211    0.851992263020   -74.6
 295    11.182327511286    0.851981862216   -58.2
 296    11.182327511286    0.851961060608   -111.7
 297    11.182330177133    0.851960938723   -46.5
 298    11.182328910856    0.851960964116   -75.6
 299    11.182327765375    0.851961046959   -40.2
 300    11.182327518738    0.851961060124   -2.8
 301    11.182327511402    0.851961060600   -77.5
 302    11.182327511286    0.851961060608   -182.5
 303    11.182327511285    0.851961060608   -39.8
 304    11.182327511286    0.851961060608   -39.7
 305    11.182327511286    0.851961060608   -5.4
 306    11.182327511286    0.851961060608   -74.4
 307    11.182327511286    0.851961060608   -39.6
 308    11.182327511286    0.851961060608   -78.4
 309    11.182327511286    0.851961060608   -38.5
 310    11.182327511286    0.851961060608   -78.5
 311    11.182327511286    0.851961060608   -17.1
 312    11.182327511286    0.851961060608   -58.2
Warning: Maximum number of function evaluations has been exceeded.
/home/alhaddwt/anaconda3/lib/python3.7/site-packages/scipy/optimize/_basinhopping.py:679: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  if isinstance(callback, collections.Callable):
requested number of basinhopping iterations completed successfully
[11.18232751  0.85196106]
optimization result for a batch of size 5 initialized at [11.   0.4] is [11.18232751  0.85196106] with functional value -182.4739683666093 with message requested number of basinhopping iterations completed successfully results save in fit_SDE19-10-01-12-40-43
 results save in fit_SDE19-10-01-12-40-43
startimg fit for a batch of size 10 initialized at [11.18232751  0.85196106]
starting optimization
Evaluating using rand_approx_lamperti
/home/alhaddwt/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py:563: RuntimeWarning: invalid value encountered in subtract
  numpy.max(numpy.abs(fsim[0] - fsim[1:])) <= fatol):
/home/alhaddwt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
 313    11.182327511286    0.851961060608   -165.1
Warning: Maximum number of function evaluations has been exceeded.
Warning: Maximum number of function evaluations has been exceeded.
requested number of basinhopping iterations completed successfully
[11.18232751  0.85196106]
optimization result for a batch of size 10 initialized at [11.18232751  0.85196106] is [11.18232751  0.85196106] with functional value -165.11367091747547 with message requested number of basinhopping iterations completed successfully results save in fit_SDE19-10-01-12-40-43
 results save in fit_SDE19-10-01-12-40-43
startimg fit for a batch of size 20 initialized at [11.18232751  0.85196106]
starting optimization
Evaluating using rand_approx_lamperti
Warning: Maximum number of function evaluations has been exceeded.
Warning: Maximum number of function evaluations has been exceeded.
/home/alhaddwt/anaconda3/lib/python3.7/site-packages/scipy/optimize/_basinhopping.py:314: RuntimeWarning: invalid value encountered in double_scalars
  w = math.exp(min(0, -float(energy_new - energy_old) * self.beta))
requested number of basinhopping iterations completed successfully
[11.18232751  0.85196106]
optimization result for a batch of size 20 initialized at [11.18232751  0.85196106] is [11.18232751  0.85196106] with functional value inf with message requested number of basinhopping iterations completed successfully results save in fit_SDE19-10-01-12-40-43
 results save in fit_SDE19-10-01-12-40-43
startimg fit for a batch of size 40 initialized at [11.18232751  0.85196106]
starting optimization
Evaluating using rand_approx_lamperti
Warning: Maximum number of function evaluations has been exceeded.
Warning: Maximum number of function evaluations has been exceeded.
requested number of basinhopping iterations completed successfully
[11.18232751  0.85196106]
optimization result for a batch of size 40 initialized at [11.18232751  0.85196106] is [11.18232751  0.85196106] with functional value inf with message requested number of basinhopping iterations completed successfully results save in fit_SDE19-10-01-12-40-43
 results save in fit_SDE19-10-01-12-40-43
startimg fit for a batch of size 80 initialized at [11.18232751  0.85196106]
starting optimization
Evaluating using rand_approx_lamperti
Warning: Maximum number of function evaluations has been exceeded.
Warning: Maximum number of function evaluations has been exceeded.
requested number of basinhopping iterations completed successfully
[11.18232751  0.85196106]
optimization result for a batch of size 80 initialized at [11.18232751  0.85196106] is [11.18232751  0.85196106] with functional value inf with message requested number of basinhopping iterations completed successfully results save in fit_SDE19-10-01-12-40-43
 results save in fit_SDE19-10-01-12-40-43
startimg fit for a batch of size 160 initialized at [11.18232751  0.85196106]
starting optimization
Evaluating using rand_approx_lamperti
Warning: Maximum number of function evaluations has been exceeded.
